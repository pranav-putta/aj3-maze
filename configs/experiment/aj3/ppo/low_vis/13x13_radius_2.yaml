# @package _global_

defaults:
  - /experiment/aj3/ppo/base_ppo

trainer:
  num_rollout_steps: 125
  epochs: 10000

  env:
    size: [ 13, 13 ]
    static_episode: False
    visible_radius: 2
    reward_type: 'distance_to_goal'
    max_steps: 125

  agent:
    policy:
      embd_dim: 32
      hidden_dim: 256
      rnn:
        layers: 2
    epsilon: 0.2
    ppo_epochs: 2
    num_minibatches: 2

    val_loss_coef: 0.4
    entropy_coef: 0.15
    max_grad_norm: 0.75
    lr: 2.5e-4